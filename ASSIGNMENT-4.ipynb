{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d639369c-ca1c-4d14-94cc-16edd4ff44a5",
   "metadata": {},
   "source": [
    "# ASSIGNMENT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a474c-4f6e-4a2a-a442-73229b2817d5",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f959b0-63eb-4f19-a304-79e57b971a1e",
   "metadata": {},
   "source": [
    "### IMPORTING DATA FROM CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbc29d5f-798f-4a01-88be-e5acf681fcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5990</th>\n",
       "      <td>598128</td>\n",
       "      <td>@michaelsheen flange huh i will remember that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>363196</td>\n",
       "      <td>@K7vans Yeah it's 7:00AM here. Still too early...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7635</th>\n",
       "      <td>761676</td>\n",
       "      <td>Stuck doing a tonnnn of homework.. Fun fun... ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8554</th>\n",
       "      <td>800554</td>\n",
       "      <td>Every time I see my Facebook profile, I imagin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8296</th>\n",
       "      <td>800296</td>\n",
       "      <td>Avoid anxiety and #depression by knowing these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>640599</td>\n",
       "      <td>had the best time at Dana's birthday party</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>256041</td>\n",
       "      <td>I got a new Tattoo yesterday it is a mama lion...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7153</th>\n",
       "      <td>713133</td>\n",
       "      <td>welcome to work: 232 mejlÅ¯ aÅ¾ jsem si z toho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>715016</td>\n",
       "      <td>Watching ze Lakers game 5.  &amp;amp;feeling 'Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>297287</td>\n",
       "      <td>@rosiebunny  No trust me, it'll work for every...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index                                 message to examine  \\\n",
       "5990  598128  @michaelsheen flange huh i will remember that ...   \n",
       "3583  363196  @K7vans Yeah it's 7:00AM here. Still too early...   \n",
       "7635  761676  Stuck doing a tonnnn of homework.. Fun fun... ...   \n",
       "8554  800554  Every time I see my Facebook profile, I imagin...   \n",
       "8296  800296  Avoid anxiety and #depression by knowing these...   \n",
       "6425  640599        had the best time at Dana's birthday party    \n",
       "2509  256041  I got a new Tattoo yesterday it is a mama lion...   \n",
       "7153  713133  welcome to work: 232 mejlÅ¯ aÅ¾ jsem si z toho...   \n",
       "7163  715016  Watching ze Lakers game 5.  &amp;feeling 'Just...   \n",
       "2911  297287  @rosiebunny  No trust me, it'll work for every...   \n",
       "\n",
       "      label (depression result)  \n",
       "5990                          0  \n",
       "3583                          0  \n",
       "7635                          0  \n",
       "8554                          1  \n",
       "8296                          1  \n",
       "6425                          0  \n",
       "2509                          0  \n",
       "7153                          0  \n",
       "7163                          0  \n",
       "2911                          0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"sentiment_tweets3.csv\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac5d65-7e8d-4c05-8bac-bb9596b7ffad",
   "metadata": {},
   "source": [
    "### OPINION LEXICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e73824-7f90-41c3-8ece-baac69459897",
   "metadata": {},
   "source": [
    "The Opinion Lexicon contains lists of positive and negative words, which are used to assign sentiment scores to text data. \n",
    "This lexicon serves as a foundational resource for sentiment analysis tasks, aiding in the identification of sentiment-bearing words within text data.\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f5b4592-91d3-47b3-998d-7abedb91e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in opinion lexicon 6789\n",
      "Examples of positive words in opinion lexicon ['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n",
      "Examples of negative words in opinion lexicon ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print('Total number of words in opinion lexicon', len(opinion_lexicon.words()))\n",
    "print('Examples of positive words in opinion lexicon',\n",
    "      opinion_lexicon.positive()[:10])\n",
    "print('Examples of negative words in opinion lexicon',\n",
    "      opinion_lexicon.negative()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e610b4d-66ce-4285-a0ad-b5737a8d7cfb",
   "metadata": {},
   "source": [
    "### CREATING A DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba22ee-1fa3-41f5-92a5-515a33bddc54",
   "metadata": {},
   "source": [
    "Here we initializes a sentiment dictionary using the Opinion Lexicon from NLTK. Positive and negative words from the lexicon are assigned sentiment scores and stored in a dictionary, preparing for sentiment analysis of review text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f243271d-f074-4955-b6c1-60c865c8b9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Let's create a dictionary which we can use for scoring our review text\n",
    "nltk.download('punkt')\n",
    "df.rename(columns={\"message to examine\": \"text\"}, inplace=True)\n",
    "pos_score = 1\n",
    "neg_score = -1\n",
    "word_dict = {}\n",
    " \n",
    "# Adding the positive words to the dictionary\n",
    "for word in opinion_lexicon.positive():\n",
    "        word_dict[word] = pos_score\n",
    "      \n",
    "# Adding the negative words to the dictionary\n",
    "for word in opinion_lexicon.negative():\n",
    "        word_dict[word] = neg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa54a07-d2ea-4652-9d2a-4a80d45e06ee",
   "metadata": {},
   "source": [
    "### BING-LIU LEXCION SCORING ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822f96-725c-4529-ac8b-e08ac2452f8a",
   "metadata": {},
   "source": [
    "Here we calculate the sentiment by using bing-liu algorithm where the algorithm tokenizes each word into bag of words and check if they are present in dictionary ,if they are present then it is added to overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1911acfd-258f-4305-bcf4-ec6c8ab33585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bing_liu_score(text):\n",
    "    sentiment_score = 0\n",
    "    bag_of_words = word_tokenize(text.lower())\n",
    "    for word in bag_of_words:\n",
    "        if word in word_dict:\n",
    "            sentiment_score += word_dict[word]\n",
    "    return sentiment_score  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a90f38-e912-44e1-9e55-9b24aff063ac",
   "metadata": {},
   "source": [
    "### FILLING NULL VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195b885-5119-47aa-bd91-770128ab7a40",
   "metadata": {},
   "source": [
    "we fill the null or empty spaces of the dataset by 'no review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12e69bd3-c097-4939-9a02-e4ecdb98b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text'] = df['text'].fillna('no review')\n",
    "\n",
    "df['Bing_Liu_Score'] = df['text'].apply(bing_liu_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e352ac-2527-4cbc-89e4-ee32165267bf",
   "metadata": {},
   "source": [
    "### Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a249666-9877-4779-a1f9-dd9e80e8aaf0",
   "metadata": {},
   "source": [
    "this line of code is used to print the first 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e26e6b2d-c27f-4c28-aa72-c10ac8cc08c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label (depression result)</th>\n",
       "      <th>text</th>\n",
       "      <th>Bing_Liu_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>so sleepy. good times tonight though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@SilkCharm re: #nbn as someone already said, d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>23 or 24ï¿½C possible today. Nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>nite twitterville  workout in the am  -ciao</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@daNanner Night, darlin'!  Sweet dreams to you</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label (depression result)  \\\n",
       "0                          0   \n",
       "1                          0   \n",
       "2                          0   \n",
       "3                          0   \n",
       "4                          0   \n",
       "5                          0   \n",
       "6                          0   \n",
       "7                          0   \n",
       "8                          0   \n",
       "9                          0   \n",
       "\n",
       "                                                text  Bing_Liu_Score  \n",
       "0  just had a real good moment. i missssssssss hi...               1  \n",
       "1         is reading manga  http://plurk.com/p/mzp1e               0  \n",
       "2  @comeagainjen http://twitpic.com/2y2lx - http:...               0  \n",
       "3  @lapcat Need to send 'em to my accountant tomo...               0  \n",
       "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder               0  \n",
       "5              so sleepy. good times tonight though                1  \n",
       "6  @SilkCharm re: #nbn as someone already said, d...               0  \n",
       "7                 23 or 24ï¿½C possible today. Nice                1  \n",
       "8        nite twitterville  workout in the am  -ciao               0  \n",
       "9    @daNanner Night, darlin'!  Sweet dreams to you                1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['label (depression result)',\"text\", 'Bing_Liu_Score']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee74789-f452-4cd3-8585-7e649a3dd459",
   "metadata": {},
   "source": [
    "### GROUPBY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f423a-86ac-4083-8d25-84670efa18d0",
   "metadata": {},
   "source": [
    "This code provides an average sentiment score for reviews categorized by their overall rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a9b16e4-5f14-4611-a0d7-d3ea0d8f874b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bing_Liu_Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label (depression result)</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.408384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Bing_Liu_Score\n",
       "label (depression result)                \n",
       "0                                0.557500\n",
       "1                               -1.408384"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label (depression result)').agg({'Bing_Liu_Score':'mean'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0058d-7fde-43c6-bfc1-3bc007d0a6ba",
   "metadata": {},
   "source": [
    "## VADER ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0ef46-eb5e-4aa7-a80d-98a359e9ea76",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a sentiment analysis algorithm designed for text, particularly social media. It evaluates sentiment by considering the intensity of positive and negative words, along with features like punctuation and emoticons, providing a compound sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e130b1-e28e-46ad-b9b1-78c77686b68f",
   "metadata": {},
   "source": [
    "similar to bing-liu algorithm vader is also widely used for sentiment analysis  where we get vaderscoring considering intensity of positive ,negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e57ffd4-6677-48e9-a310-98fadedc0eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  VADER_Score\n",
      "0      just had a real good moment. i missssssssss hi...       0.4404\n",
      "1             is reading manga  http://plurk.com/p/mzp1e       0.0000\n",
      "2      @comeagainjen http://twitpic.com/2y2lx - http:...       0.0000\n",
      "3      @lapcat Need to send 'em to my accountant tomo...       0.4404\n",
      "4          ADD ME ON MYSPACE!!!  myspace.com/LookThunder       0.0000\n",
      "...                                                  ...          ...\n",
      "10309  No Depression by G Herbo is my mood from now o...      -0.8126\n",
      "10310  What do you do when depression succumbs the br...      -0.2960\n",
      "10311  Ketamine Nasal Spray Shows Promise Against Dep...      -0.7845\n",
      "10312  dont mistake a bad day with depression! everyo...       0.1950\n",
      "10313                                                  0       0.0000\n",
      "\n",
      "[10314 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def vader_score(text):\n",
    "    # Analyze sentiment of the text\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "df['VADER_Score'] = df['text'].apply(vader_score)\n",
    "print(df[['text', 'VADER_Score']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46806e8-4478-41d3-91fa-40b6cb64f977",
   "metadata": {},
   "source": [
    "## TEXTBLOB ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cb29e-ddc8-4ff3-9785-7bad14ad54d3",
   "metadata": {},
   "source": [
    "TextBlob relies on a lexicon and machine learning,TextBlob adds up the sentiment polarity scores of individual words to calculate the overall sentiment polarity score for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7504200-6075-45bb-a1c0-442f299f8659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      message to examine  TextBlob_Score\n",
      "0      just had a real good moment. i missssssssss hi...        0.600000\n",
      "1             is reading manga  http://plurk.com/p/mzp1e        0.000000\n",
      "2      @comeagainjen http://twitpic.com/2y2lx - http:...        0.000000\n",
      "3      @lapcat Need to send 'em to my accountant tomo...        0.041667\n",
      "4          ADD ME ON MYSPACE!!!  myspace.com/LookThunder        0.000000\n",
      "...                                                  ...             ...\n",
      "10309  No Depression by G Herbo is my mood from now o...        0.000000\n",
      "10310  What do you do when depression succumbs the br...        0.000000\n",
      "10311  Ketamine Nasal Spray Shows Promise Against Dep...        0.000000\n",
      "10312  dont mistake a bad day with depression! everyo...       -1.000000\n",
      "10313                                                  0        0.000000\n",
      "\n",
      "[10314 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_score(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    # Get the sentiment polarity\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    return sentiment_score\n",
    "df = pd.read_csv(\"sentiment_tweets3.csv\")\n",
    "df['TextBlob_Score'] = df['message to examine'].apply(textblob_score)  \n",
    "\n",
    "print(df[['message to examine', 'TextBlob_Score']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c82aff-6b5f-4886-8ac8-afb44c7c596a",
   "metadata": {},
   "source": [
    "### SENTIWORDNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3df2b-7678-4f5e-9f74-64e56b740840",
   "metadata": {},
   "source": [
    "SentiWordNet relies on synsets in WordNet to assign sentiment scores to words.SentiWordNet works by assigning sentiment scores to words based on their synsets (sets of synonyms representing a concept) in WordNet. It provides scores for both positivity and negativity, allowing for nuanced sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f4c1873-21c5-4f66-95c9-f81d093956f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6756fc19-e79e-416e-9f80-3023c2f5eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      message to examine  SentiWordNet_Score\n",
      "0      just had a real good moment. i missssssssss hi...            0.100000\n",
      "1             is reading manga  http://plurk.com/p/mzp1e            0.000000\n",
      "2      @comeagainjen http://twitpic.com/2y2lx - http:...            0.000000\n",
      "3      @lapcat Need to send 'em to my accountant tomo...            0.045455\n",
      "4          ADD ME ON MYSPACE!!!  myspace.com/LookThunder            0.500000\n",
      "...                                                  ...                 ...\n",
      "10309  No Depression by G Herbo is my mood from now o...            0.035714\n",
      "10310  What do you do when depression succumbs the br...            0.031250\n",
      "10311  Ketamine Nasal Spray Shows Promise Against Dep...            0.027778\n",
      "10312  dont mistake a bad day with depression! everyo...           -0.468750\n",
      "10313                                                  0            0.000000\n",
      "\n",
      "[10314 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize WordNet Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def sentiwordnet_score(text):\n",
    "    total_score = 0\n",
    "    word_count = 0\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    for word, pos_tag in nltk.pos_tag(tokenized_text):\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        synsets = list(swn.senti_synsets(word))\n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            total_score += synset.pos_score() - synset.neg_score()\n",
    "            word_count += 1\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return total_score / word_count\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"sentiment_tweets3.csv\") \n",
    "df['SentiWordNet_Score'] = df['message to examine'].apply(sentiwordnet_score)  \n",
    "print(df[['message to examine', 'SentiWordNet_Score']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e481211-5917-4b3f-bfb3-7d1d35af672e",
   "metadata": {},
   "source": [
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590ea80-203b-408e-be54-0130dceac3df",
   "metadata": {},
   "source": [
    "Here, I used four lexicon scoring algorithms for doing sentiment analysis on tweets dataset those include bing-liu lexcion scoring,sentiwordnet,textblob and vader algorithms.\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner):\r",
    ": VADER is often preferred for its simplicity, speed, and effectiveness in analyzing sentiment from social media text and short informal messages. It doesn't require training data and provides sentiment scores based on lexicons and grammatical rule\n",
    "\n",
    "Out of all as my datase was a tweet dataset i.e social media related which had a high possibility of having informal messages.VADER algorithm is considered as a good choice.ay suffice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
